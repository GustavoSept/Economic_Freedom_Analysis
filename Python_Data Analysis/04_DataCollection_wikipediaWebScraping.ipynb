{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Wikipedia Data\n",
    "We'll extract some information about border lenghts and neighbours, wars, basic infrastructure, and general info like Top religions, area in KM, type of government and country position. \n",
    "### Country Border information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from fuzzywuzzy import process\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll use a processed Data Set from an earlier notebook as the stardard for Country Names\n",
    "df = pd.read_csv(\"..\\Data_Sets\\processed\\economicData_1960-2022_noNaN-drops.csv\") \n",
    "refNames = pd.DataFrame({\n",
    "    'Standard Names': df['Country Name'].unique()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "PARAMS = {\n",
    "    \"action\": \"parse\",\n",
    "    \"page\": \"List_of_countries_and_territories_by_number_of_land_borders\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "response = requests.get(BASE_URL, params=PARAMS)\n",
    "data = response.json()\n",
    "\n",
    "# The main content of the page is in ['parse']['text']['*']\n",
    "page_html = data['parse']['text']['*']\n",
    "\n",
    "# We can now use BeautifulSoup to parse this HTML\n",
    "soup = BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "# Find the table with neighbouring countries info (which is the first)\n",
    "table = soup.find('table')\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzySearchName(name, refNames=refNames, score_threshold = 45):\n",
    "    '''\n",
    "        Given a country name, returns the closest match from refNames using fuzzy search.\n",
    "        If no close match is found, it returns \"PLEASE FILL MANUALLY\".\n",
    "    '''\n",
    "    \n",
    "    # If the name is empty or null, return \"UNKNOWN\"\n",
    "    if pd.isna(name) or not name.strip():\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    match, score, _ = process.extractOne(name, refNames['Standard Names'])\n",
    "    \n",
    "    # Hardcoding exceptions, due to sharing common words (South, North)\n",
    "    if match in ['North Korea', 'South Korea', 'South Africa', 'North Macedonia']:\n",
    "        score_threshold = max(89, score_threshold)\n",
    "        \n",
    "\n",
    "    # If a close match is found, return the match\n",
    "    if score > score_threshold:\n",
    "        return match\n",
    "    \n",
    "    # If no close match is found, ask for manual input\n",
    "    return \"PLEASE FILL MANUALLY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "countriesBordersList = {}\n",
    "for tr in table.findAll('tr')[2:]: # Skip the 2 lines-header row by using slicing\n",
    "    tds = tr.findAll('td')\n",
    "    \n",
    "    countryName = tds[0].find('b').find('a').text \n",
    "    \n",
    "    neighboursBorders_inKM = tds[1].text.strip()    \n",
    "    neighbouring_countries = []\n",
    "    \n",
    "    # Extracting neighbouring countries\n",
    "    links_in_td = tds[5].findAll('a')\n",
    "    if links_in_td:\n",
    "        for a in links_in_td:\n",
    "            if '[' not in a.text: # Excluding reference links\n",
    "                currentName = a.text.strip()\n",
    "                currentName = fuzzySearchName(currentName) # Correcting names with fuzzySearch\n",
    "                \n",
    "                if currentName in refNames['Standard Names'].values: # Discarding mismatchs, only interested in one of the 185 Countries\n",
    "                    neighbouring_countries.append(currentName)\n",
    "\n",
    "    neighbouring_countries = list(set(neighbouring_countries))\n",
    "    # Adding countryName as Key and neighbouring_countries as Value in countriesBordersList dict\n",
    "    countriesBordersList[countryName] = {\n",
    "        'Borders Length (in KM)': neighboursBorders_inKM,\n",
    "        'Neighbouring Countries': neighbouring_countries\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivoting the dictionary, before creating the dataFrame\n",
    "data_list = [{'Country Name': country, **values} for country, values in countriesBordersList.items()]\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df_CountryNeigh = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Country Name with the standard i'm using in other Data Sets\n",
    "df_CountryNeigh['New Country Name'] = df_CountryNeigh['Country Name'].apply(fuzzySearchName)\n",
    "\n",
    "df_CountryNeigh.loc[df_CountryNeigh['Country Name'] == 'Kyrgyzstan', 'New Country Name'] = 'Kyrgyz Republic'\n",
    "df_CountryNeigh.loc[df_CountryNeigh['Country Name'] == 'Slovakia', 'New Country Name'] = 'Slovak Republic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 250)\n",
    "pd.set_option('display.min_rows', 135)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Cleaning Countries in the dataFrame:\n",
    "# - Removing duplicates (where the original Country Name isn't in refNames)\n",
    "# - The only exception is Bahamas, otherwise all other countries work.\n",
    "mask_oldToActual = df_CountryNeigh['Country Name'].isin(refNames['Standard Names'])\n",
    "mask_duplicatedEntries = df_CountryNeigh['New Country Name'].duplicated(keep=False)\n",
    "condition = (~mask_oldToActual) & (mask_duplicatedEntries) & (df_CountryNeigh['Country Name'] != 'Bahamas')\n",
    "\n",
    "df_CountryNeigh = df_CountryNeigh[~condition]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing empty row\n",
    "df_CountryNeigh = df_CountryNeigh[~(df_CountryNeigh['New Country Name'] == 'UNKNOWN')]\n",
    "\n",
    "# Removing old names, and renaming New Country Name column\n",
    "df_CountryNeigh.drop('Country Name', axis = 1, inplace=True)\n",
    "df_CountryNeigh.rename(columns = {'New Country Name': 'Country Name'}, inplace=True)\n",
    "\n",
    "\n",
    "# Manually adding Kosovo, which is missing from the wikipedia page\n",
    "kosovo = {\n",
    "    'Country Name': 'Kosovo',\n",
    "    'Borders Length (in KM)': 743.556,\n",
    "    'Neighbouring Countries': ['Albania', 'Montenegro', 'North Macedonia', 'Serbia']\n",
    "}\n",
    "df_CountryNeigh = pd.concat([df_CountryNeigh, pd.DataFrame([kosovo])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### War information\n",
    "We'll gather all wars between countries (and also significant Civil Wars/Rebellions) from 1900s onward. We'll need to clean them while collecting, as only Wars with recognizable states will be accepted. We'll discard all other instances.\n",
    "\n",
    "This means this probably won't be as thorough/accurate as manual data insertion, but it'll take us 90% there with a fraction of the effort.\n",
    "With such info, we'll estimate:\n",
    "- Distinct Count of neighbours each country has warred with (This means we'll need to filter Civil Wars [Wars with self] for this one)\n",
    "- Count of Wars each country has had with neighbours\n",
    "- Total Count of Wars each country has had"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE TO SELF\n",
    "I'll need to keep side A and B of the war (doesn't matter which is winning or losing). So that I can calculate the above list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping relevant names to a dataFrame, so our fuzzySearch will work properly\n",
    "expRefNames = pd.DataFrame({\n",
    "    'Standard Names': ['Ottoman', 'Weimar Republic', 'Qing Dynasty', 'Bitterenders', \"Ha'il\", \"Ikhwan\", \"Najran\", \"British Empire\",\n",
    "                       \"England\", 'Kurdistan', \"French\", \"Soviet Union\", \"Kurdish\", \"Ararat\", \"Khan\", \"Saqqawists\", \"Khanty\", \"Muhammad Umar\",\n",
    "                       \"Spanish\", \"Polish\", \"Palestine\", \"Czechoslovakia\", \"Rhodesia\", \"Zaire\", \"Turkistan\", \"hamas\"],\n",
    "    'targetNames': ['Turkey', 'Germany', 'China', 'South Africa', \"Saudi Arabia\", \"Saudi Arabia\", \"Saudi Arabia\", \"United Kingdom\",\n",
    "                    \"United Kingdom\", 'Iraq', \"France\", \"Russia\", \"Turkey\", \"Turkey\", \"Afghanistan\", \"Afghanistan\", \"Russia\", \"Kazakhstan\",\n",
    "                    \"Spain\", \"Poland\", \"Israel\", \"Czech Republic\", \"Zimbabwe\", \"Democratic Republic of Congo\", \"Pakistan\", \"Israel\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS = {\n",
    "    \"action\": \"parse\",\n",
    "    \"page\": \"Lists_of_wars\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "response = requests.get(BASE_URL, params=PARAMS)\n",
    "response.status_code\n",
    "\n",
    "data = response.json()\n",
    "page_html = data['parse']['text']['*']\n",
    "soup = BeautifulSoup(page_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all past wars links\n",
    "pastWars_links = soup.findAll('a', string=re.compile(\"List of wars:\"))\n",
    "pastWars_linkList = []\n",
    "\n",
    "# Inserting links from 1900s onward into list\n",
    "for i, link in enumerate(pastWars_links):\n",
    "    if i > 3:\n",
    "        pastWars_linkList.append(link.get('href')[6:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now iterate over all tables from the given links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Status_Code: 200'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fixing URLs format\n",
    "pastWars_linkList = [link.replace(f\"%E2%80%93\", '–') for link in pastWars_linkList]\n",
    "\n",
    "TEMP_ITERATOR = 0\n",
    "for link in pastWars_linkList:\n",
    "    # For each link, find all tables\n",
    "    \n",
    "    if TEMP_ITERATOR > 0:\n",
    "        break\n",
    "    TEMP_ITERATOR += 1\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"parse\",\n",
    "        \"page\": link,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = requests.get(BASE_URL, params=PARAMS)\n",
    "\n",
    "    if response.status_code//100 != 2: # if status code isn't in the 200s\n",
    "        display(f'Something unexpected happened. Status Code returned: {response.status_code}')\n",
    "        break\n",
    "    else:\n",
    "        display(f'Status_Code: {response.status_code}')\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    if 'parse' in data:\n",
    "        page_html = data['parse']['text']['*']\n",
    "        soup = BeautifulSoup(page_html, 'html.parser')\n",
    "        tables = soup.findAll('table')\n",
    "    else:\n",
    "        display('------------------------- ERROR ------------------------- ')\n",
    "        display(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for table in tables:\n",
    "#    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EconStats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
