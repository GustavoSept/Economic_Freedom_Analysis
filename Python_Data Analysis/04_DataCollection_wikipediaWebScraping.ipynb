{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Wikipedia Data\n",
    "We'll extract some information about border lenghts and neighbours, wars, basic infrastructure, and general info like Top religions, area in KM, and country position.\n",
    "\n",
    "### Preparations\n",
    "Let's write some functions and lists to make our jobs easier for the next tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from fuzzywuzzy import process\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll use a processed Data Set from an earlier notebook as the stardard for Country Names\n",
    "df = pd.read_csv(\"..\\Data_Sets\\processed\\economicData_1960-2022_noNaN-drops.csv\") \n",
    "refNames = pd.DataFrame({    \n",
    "    'Standard Names': df['Country Name'].unique(),\n",
    "    'targetNames': df['Country Name'].unique()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSoupFromWiki(\n",
    "        link,\n",
    "        BASE_URL = \"https://en.wikipedia.org/w/api.php\",\n",
    "        action = \"parse\",\n",
    "        format = \"json\"\n",
    "    ):\n",
    "\n",
    "    '''\n",
    "        Given a title, it searches a link and returns a soup.\n",
    "    '''\n",
    "\n",
    "    params = {\n",
    "        \"action\": action,\n",
    "        \"page\": link,\n",
    "        \"format\": format\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "    def isReponseOK(response=response):\n",
    "        if response.status_code//100 != 2: # if status code isn't in the 200s\n",
    "            display(f'Something unexpected happened. Status Code returned: {response.status_code}')\n",
    "            return False\n",
    "        else:\n",
    "            # display(f'Successful Response: {response.status_code}')\n",
    "            # display(f'Processing current link: {link}')\n",
    "            return True\n",
    "\n",
    "    if not isReponseOK():\n",
    "        return []\n",
    "\n",
    "    def htmlParser_getSoup(response):\n",
    "        data = response.json()\n",
    "\n",
    "        # The main content of the page is in ['parse']['text']['*']\n",
    "        page_html = data['parse']['text']['*']\n",
    "\n",
    "        # We can now use BeautifulSoup to parse this HTML\n",
    "        soup = BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "        return soup\n",
    "\n",
    "    soup = htmlParser_getSoup(response)\n",
    "\n",
    "    # Check for redirect indication, and open the first valid link\n",
    "    if \"This is a redirect from a title\" in soup.text or \"Redirect to:\" in soup.text:\n",
    "        newTitle = soup.find(\"a\").text\n",
    "        if newTitle:\n",
    "            params['page'] = newTitle\n",
    "\n",
    "            response = requests.get(BASE_URL, params=params)\n",
    "            #display(f'Redirecting from {link} to {newTitle}')\n",
    "            #display('----------------------------------------')\n",
    "            return htmlParser_getSoup(response)         \n",
    "    \n",
    "    #display('----------------------------------------')\n",
    "    return soup\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzySearchName(name, refNames=refNames, score_threshold = 45):\n",
    "    '''\n",
    "        Given a country name, returns the closest match from refNames using fuzzy search.\n",
    "        If no close match is found, it returns \"PLEASE FILL MANUALLY\".\n",
    "    '''\n",
    "    \n",
    "    # If the name is empty or null, return \"UNKNOWN\"\n",
    "    if pd.isna(name) or not name.strip():\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    match, score, _ = process.extractOne(name, refNames['Standard Names'])\n",
    "    \n",
    "    # Hardcoding exceptions, due to sharing common words (South, North)\n",
    "    dubiousFuzzyNames = [\n",
    "        'North Korea', 'South Korea', 'South Africa', 'North Macedonia',\n",
    "        'Democratic Republic of Congo', 'Republic of Congo', 'British Empire'\n",
    "    ]\n",
    "\n",
    "    if match in dubiousFuzzyNames:\n",
    "        score_threshold = max(89, score_threshold)\n",
    "        \n",
    "\n",
    "    # If a close match is found, return the match\n",
    "    if score > score_threshold:\n",
    "        return match\n",
    "    \n",
    "    # If no close match is found, ask for manual input\n",
    "    return \"PLEASE FILL MANUALLY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping relevant names to a dataFrame, so our fuzzySearch will work properly\n",
    "expRefNames = pd.DataFrame({\n",
    "    'Standard Names': ['Ottoman Empire', 'Weimar Republic', 'Qing Dynasty', 'Bitterenders', \"Ha'il\", \"Ikhwan\", \"Najran\", \"British Empire\",\n",
    "                       \"England\", 'Kurdistan', \"French\", \"Soviet Union\", \"Kurdish\", \"Ararat\", \"Khan\", \"Saqqawists\", \"Khanty\", \"Muhammad Umar\",\n",
    "                       \"Spanish\", \"Polish\", \"Palestine\", \"Czechoslovakia\", \"Rhodesia\", \"Zaire\", \"Turkistan\", \"hamas\", 'Dutch Empire', 'Portuguese',\n",
    "                       \"Czechia\", \"Kyrgyzstan\", \"Slovakia\"],\n",
    "    'targetNames': ['Turkey', 'Germany', 'China', 'South Africa', \"Saudi Arabia\", \"Saudi Arabia\", \"Saudi Arabia\", \"United Kingdom\",\n",
    "                    \"United Kingdom\", 'Iraq', \"France\", \"Russia\", \"Turkey\", \"Turkey\", \"Afghanistan\", \"Afghanistan\", \"Russia\", \"Kazakhstan\",\n",
    "                    \"Spain\", \"Poland\", \"Israel\", \"Czech Republic\", \"Zimbabwe\", \"Democratic Republic of Congo\", \"Pakistan\", \"Israel\", 'Netherlands', 'Portugal',\n",
    "                    \"Czech Republic\", \"Kyrgyz Republic\", \"Slovak Republic\"]\n",
    "})\n",
    "\n",
    "# Concatenate the dataframes along columns\n",
    "merged_refNames = pd.concat([refNames, expRefNames], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country Border information\n",
    "We're extracting:\n",
    "- For every country, who their neighbouring countries are\n",
    "- What is the length of the borders in KM (not meaning the circumference of the country, it counts just the parts that touches other countries by Land)\n",
    "    - This means that, at least for our purposes, this data could be more complete. As what (could) actually matter for our model is if a country is close to another, not necessarily touching eachother.\n",
    "    - That is why we'll later join this info with country position, so we'll add more countries to the list, with some sort of distance threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = getSoupFromWiki(\"List_of_countries_and_territories_by_number_of_land_borders\")\n",
    "\n",
    "# Find the table with neighbouring countries info (which is the first)\n",
    "table = soup.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "countriesBordersList = {}\n",
    "for tr in table.findAll('tr')[2:]: # Skip the 2 lines-header row by using slicing\n",
    "    tds = tr.findAll('td')\n",
    "    \n",
    "    countryName = tds[0].find('b').find('a').text \n",
    "    \n",
    "    neighboursBorders_inKM = tds[1].text.strip()    \n",
    "    neighbouring_countries = []\n",
    "    \n",
    "    # Extracting neighbouring countries\n",
    "    links_in_td = tds[5].findAll('a')\n",
    "    if links_in_td:\n",
    "        for a in links_in_td:\n",
    "            if '[' not in a.text: # Excluding reference links\n",
    "                currentName = a.text.strip()\n",
    "                currentName = fuzzySearchName(currentName) # Correcting names with fuzzySearch\n",
    "                \n",
    "                if currentName in refNames['Standard Names'].values: # Discarding mismatchs, only interested in one of the 185 Countries\n",
    "                    neighbouring_countries.append(currentName)\n",
    "\n",
    "    neighbouring_countries = list(set(neighbouring_countries))\n",
    "    # Adding countryName as Key and neighbouring_countries as Value in countriesBordersList dict\n",
    "    countriesBordersList[countryName] = {\n",
    "        'Borders Length (in KM)': neighboursBorders_inKM,\n",
    "        'Neighbouring Countries': neighbouring_countries\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivoting the dictionary, before creating the dataFrame\n",
    "data_list = [{'Country Name': country, **values} for country, values in countriesBordersList.items()]\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df_CountryNeigh = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Country Name with the standard i'm using in other Data Sets\n",
    "df_CountryNeigh['New Country Name'] = df_CountryNeigh['Country Name'].apply(fuzzySearchName)\n",
    "\n",
    "df_CountryNeigh.loc[df_CountryNeigh['Country Name'] == 'Kyrgyzstan', 'New Country Name'] = 'Kyrgyz Republic'\n",
    "df_CountryNeigh.loc[df_CountryNeigh['Country Name'] == 'Slovakia', 'New Country Name'] = 'Slovak Republic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Countries in the dataFrame:\n",
    "# - Removing duplicates (where the original Country Name isn't in refNames)\n",
    "# - The only exception is Bahamas, otherwise all other countries work.\n",
    "mask_oldToActual = df_CountryNeigh['Country Name'].isin(refNames['Standard Names'])\n",
    "mask_duplicatedEntries = df_CountryNeigh['New Country Name'].duplicated(keep=False)\n",
    "condition = (~mask_oldToActual) & (mask_duplicatedEntries) & (df_CountryNeigh['Country Name'] != 'Bahamas')\n",
    "\n",
    "df_CountryNeigh = df_CountryNeigh[~condition]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing empty row\n",
    "df_CountryNeigh = df_CountryNeigh[~(df_CountryNeigh['New Country Name'] == 'UNKNOWN')]\n",
    "\n",
    "# Removing old names, and renaming New Country Name column\n",
    "df_CountryNeigh.drop('Country Name', axis = 1, inplace=True)\n",
    "df_CountryNeigh.rename(columns = {'New Country Name': 'Country Name'}, inplace=True)\n",
    "\n",
    "\n",
    "# Manually adding Kosovo, which is missing from the wikipedia page\n",
    "kosovo = {\n",
    "    'Country Name': 'Kosovo',\n",
    "    'Borders Length (in KM)': 743.556,\n",
    "    'Neighbouring Countries': ['Albania', 'Montenegro', 'North Macedonia', 'Serbia']\n",
    "}\n",
    "df_CountryNeigh = pd.concat([df_CountryNeigh, pd.DataFrame([kosovo])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Borders Length (in KM)</th>\n",
       "      <th>Neighbouring Countries</th>\n",
       "      <th>Country Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5,529</td>\n",
       "      <td>[Iran, Turkmenistan, Tajikistan, Pakistan, Uzb...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>720</td>\n",
       "      <td>[Montenegro, Serbia, Greece, North Macedonia]</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6,470</td>\n",
       "      <td>[Tunisia, Niger, Mauritania, Mali, Libya, Moro...</td>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Borders Length (in KM)                             Neighbouring Countries  \\\n",
       "0                  5,529  [Iran, Turkmenistan, Tajikistan, Pakistan, Uzb...   \n",
       "1                    720      [Montenegro, Serbia, Greece, North Macedonia]   \n",
       "2                  6,470  [Tunisia, Niger, Mauritania, Mali, Libya, Moro...   \n",
       "\n",
       "  Country Name  \n",
       "0  Afghanistan  \n",
       "1      Albania  \n",
       "2      Algeria  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_CountryNeigh.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### War information\n",
    "We'll gather all wars between countries (and also significant Civil Wars/Rebellions) from 1900s onward. We'll need to clean them while collecting, as only Wars with recognizable states will be accepted. We'll discard all other instances.\n",
    "\n",
    "This means this probably won't be as thorough/accurate as manual data insertion, but it'll take us 90% there with a fraction of the effort.\n",
    "With such info, we'll estimate:\n",
    "- Distinct Count of neighbours each country has warred with (This means we'll need to filter Civil Wars [Wars with self] for this one)\n",
    "- Count of Wars each country has had with neighbours\n",
    "- Total Count of Wars each country has had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = getSoupFromWiki(\"Lists_of_wars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all past wars links\n",
    "pastWars_links = soup.findAll('a', string=re.compile(\"List of wars:\"))\n",
    "pastWars_linkList = []\n",
    "\n",
    "# Inserting links from 1900s onward into list\n",
    "for i, link in enumerate(pastWars_links):\n",
    "    if i > 3:\n",
    "        pastWars_linkList.append(link.get('href')[6:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now iterate over all tables from the given links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Standard Names', 'targetNames'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(merged_refNames.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current tr has failed. Skipping it this iteration.\n",
      "PS: Probably just the last row though\n",
      "Current tr has failed. Skipping it this iteration.\n",
      "PS: Probably just the last row though\n",
      "Current tr has failed. Skipping it this iteration.\n",
      "PS: Probably just the last row though\n",
      "Current tr has failed. Skipping it this iteration.\n",
      "PS: Probably just the last row though\n"
     ]
    }
   ],
   "source": [
    "# Fixing URLs format\n",
    "pastWars_linkList = [link.replace(f\"%E2%80%93\", '–') for link in pastWars_linkList]\n",
    "\n",
    "# These dicts will contain Lists as value, storing every Country that warred against the 'key' Country.\n",
    "opponentsByCountry = {}\n",
    "\n",
    "for link in pastWars_linkList:\n",
    "    # For each link, find all tables\n",
    "    soup = getSoupFromWiki(link)\n",
    "    tables = soup.findAll('table')    \n",
    "    \n",
    "    # ------------------------------------- Iterating over each \n",
    "    for table in tables:\n",
    "        # For each table, fetch all rows for parties involved (A vs B)\n",
    "        for tr in table.findAll('tr')[2:]: # Skip the 2 lines-header row by using slicing\n",
    "            tds = tr.findAll('td')\n",
    "\n",
    "            # local store, to permutate later\n",
    "            countriesA = []\n",
    "            countriesB = []\n",
    "            \n",
    "            try:\n",
    "                sideACountries = tds[3].findAll('a')\n",
    "                sideBCountries = tds[4].findAll('a')\n",
    "\n",
    "                # ---------------- Side A\n",
    "                for link in sideACountries:\n",
    "                    countryA = fuzzySearchName(link.text, merged_refNames, score_threshold = 80) # 80 is a good balance, empirically tested on this dataSet\n",
    "\n",
    "                    # If name is valid, change it's 'wikiName' to the targetName I've set before, then append it\n",
    "                    if countryA not in ['PLEASE FILL MANUALLY', 'UNKNOWN']:\n",
    "                        countryA = merged_refNames.loc[merged_refNames['Standard Names'] == countryA]['targetNames'].iloc[0]\n",
    "                        countriesA.append(countryA)\n",
    "\n",
    "                # ---------------- Side B\n",
    "                for link in sideBCountries:\n",
    "                    countryB = fuzzySearchName(link.text, merged_refNames, score_threshold = 85) # I'm more rigorous, since the losing side is more likely to have odd names\n",
    "\n",
    "                    # If name is valid, change it's 'wikiName' to the targetName I've set before, then append it\n",
    "                    if countryB not in ['PLEASE FILL MANUALLY', 'UNKNOWN']:\n",
    "                        countryB = merged_refNames.loc[merged_refNames['Standard Names'] == countryB]['targetNames'].iloc[0]\n",
    "                        countriesB.append(countryB)\n",
    "\n",
    "                # Append to the actual dictionary\n",
    "                for country in countriesA:\n",
    "                    if country in opponentsByCountry:\n",
    "                        # If we've seen the country before, only extend the list.\n",
    "                        opponentsByCountry[country].extend(countriesB)\n",
    "                    else:\n",
    "                        # Else, create a new Key with copied Values\n",
    "                        opponentsByCountry[country] = countriesB.copy()\n",
    "                        \n",
    "\n",
    "            except:\n",
    "                print(f'Current tr has failed. Skipping it this iteration.\\nPS: Probably just the last row though')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landlocked Countries\n",
    "We'll extract:\n",
    "- Which countries are landlocked\n",
    "- Neighbour count who has access to the sea (if landlocked) # I'll default 6, if the country isn't landlocked. (as this is the max value + 1, from landlocked countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid row. Error: list index out of range.\n",
      "Skipping invalid row. Error: 'NoneType' object has no attribute 'text'.\n",
      "Skipping invalid row. Error: 'NoneType' object has no attribute 'text'.\n"
     ]
    }
   ],
   "source": [
    "soup = getSoupFromWiki('Landlocked_country')\n",
    "rows = soup.find('tbody').findAll('tr')\n",
    "\n",
    "df_landLockedCountries = pd.DataFrame(columns=['Country Name', 'isLandLocked', 'n_accessToSea'])\n",
    "set_countries_landLocked = set()\n",
    "\n",
    "for row in rows[2:]:\n",
    "    tds = row.findAll('td')\n",
    "    try:\n",
    "        countryName = fuzzySearchName(tds[0].find('a').text, refNames=merged_refNames, score_threshold = 75)\n",
    "        if countryName == \"PLEASE FILL MANUALLY\" or countryName == \"UNKNOWN\":\n",
    "            continue\n",
    "\n",
    "        # Reverting alternative names back to our naming convention (targetNames)\n",
    "        countryName = merged_refNames.loc[merged_refNames['Standard Names'] == countryName]['targetNames'].iloc[0]\n",
    "\n",
    "        # Neighbours with access to the ocean\n",
    "        n_accessToSea = tds[7].text[0]\n",
    "\n",
    "        # We'll populate a set, to get countries outside of this list\n",
    "        set_countries_landLocked.add(countryName)\n",
    "\n",
    "        # Populating dataFrame\n",
    "        new_row = pd.DataFrame({\n",
    "            'Country Name': [countryName],\n",
    "            'isLandLocked': [True],\n",
    "            'n_accessToSea': [n_accessToSea]\n",
    "        })\n",
    "\n",
    "        df_landLockedCountries = pd.concat([df_landLockedCountries, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "        #print(f'Country: {countryName}  |  Neighbours with access to ocean: {n_accessToSea}')\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping invalid row. Error: {e}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shape of df_landLockedCountries: (185, 3)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>isLandLocked</th>\n",
       "      <th>n_accessToSea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country Name isLandLocked n_accessToSea\n",
       "0  Afghanistan         True             3\n",
       "1      Armenia         True             3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>isLandLocked</th>\n",
       "      <th>n_accessToSea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Panama</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Equatorial Guinea</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country Name isLandLocked n_accessToSea\n",
       "183             Panama        False             6\n",
       "184  Equatorial Guinea        False             6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now let's populate df_landLockedCountries with countries that does have access to the ocean\n",
    "\n",
    "set_countries_accessOcean = set(df['Country Name'].unique())\n",
    "\n",
    "odd = set(df_landLockedCountries['Country Name'].unique())\n",
    "\n",
    "diff = set_countries_accessOcean - set_countries_landLocked\n",
    "\n",
    "for country in diff:\n",
    "    new_row = pd.DataFrame({\n",
    "            'Country Name': [country],\n",
    "            'isLandLocked': [False],\n",
    "            'n_accessToSea': [6]\n",
    "        })\n",
    "\n",
    "    df_landLockedCountries = pd.concat([df_landLockedCountries, new_row], ignore_index=True)\n",
    "\n",
    "display(f'Shape of df_landLockedCountries: {df_landLockedCountries.shape}')\n",
    "display(df_landLockedCountries.head(2))\n",
    "display(df_landLockedCountries.tail(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Country Info\n",
    "We'll extract:\n",
    "- Top 3 religions\n",
    "    - Cleaning Strategy:\n",
    "        - Select only the first 3 elements\n",
    "        - If \\n is found, get only the first name in that string [\\n means wiki is subdividing religions into sub-groups]\n",
    "            - To Achieve this, select whatever is between one \\n from another \\n\n",
    "            - Then, we'll select only alphabetical letters from the resulting string\n",
    "        - Standardize names to reduce categories (i.e. Islam = Sunni Islam)\n",
    "- Country Position in Globe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiled regex patterns\n",
    "PERCENTAGE_PATTERN = re.compile(r'(\\d+(\\.\\d+)?)%')\n",
    "TEXT_CLEANUP_PATTERN = re.compile(r'[a-zA-Z]+')\n",
    "\n",
    "def clean_extracted_religions(data_list):\n",
    "    religions = []\n",
    "    \n",
    "    if isinstance(data_list, str):\n",
    "        data_list = list([data_list]) # Forcing a list, for consistency\n",
    "\n",
    "    for li in data_list[:3]: # Get only the top 3 items\n",
    "        try:\n",
    "            text = li.get_text()\n",
    "        except:\n",
    "            # If there's just one element, this is triggered\n",
    "            # As single elements don't have .get_text() method\n",
    "            text = li\n",
    "        # Extract between first and second \\n\n",
    "        text = text.split('\\n')[1] if '\\n' in text else text\n",
    "        \n",
    "        # Check for excluded strings\n",
    "        if any(dash in text.lower() for dash in ['—', '-', 'other', 'undeclared']):\n",
    "            continue\n",
    "        \n",
    "        # Check size of Religion. If lower than 4%, skip it\n",
    "        percentage_match = PERCENTAGE_PATTERN.search(text)\n",
    "        \n",
    "        if percentage_match:\n",
    "            percentage = float(percentage_match.group(1))\n",
    "            if percentage < 4:\n",
    "                continue\n",
    "        \n",
    "        # Remove text after ( or [\n",
    "        text = re.split(r'[\\(\\[]', text)[0].strip()\n",
    "        \n",
    "        # Extract only alphabetical characters\n",
    "        cleaned_text = ' '.join(TEXT_CLEANUP_PATTERN.findall(text))        \n",
    "\n",
    "        religions.append(cleaned_text.lower())\n",
    "    \n",
    "    return religions\n",
    "\n",
    "def get_religion_from_table(table):\n",
    "    religion_data = table.select_one('th.infobox-label:-soup-contains(\"Religion\") + td.infobox-data')\n",
    "    # display('++++++++++++++++++++++++ religion_data from get_religion_from_table function')\n",
    "    # display(religion_data)\n",
    "\n",
    "    # If there's no line element, return the first link\n",
    "    if not religion_data.find('li'):\n",
    "        allRows = religion_data.find('a').text\n",
    "        # display('\"No Lines\" method used')\n",
    "        # display(allRows)\n",
    "        return clean_extracted_religions(allRows)\n",
    "    \n",
    "    if religion_data:\n",
    "        try:\n",
    "            # Get all lines within the same hierarchical level as the first li\n",
    "            allRows = [religion_data.find('li')] + religion_data.find('li').find_next_siblings('li')\n",
    "            # display('Line method used')\n",
    "            # display(allRows)\n",
    "        except:\n",
    "            # If the method above fails, we use another approach\n",
    "            allRows = religion_data.find_all('a')\n",
    "            # display('\"All Links\" method used')\n",
    "            # display(allRows)\n",
    "        return clean_extracted_religions(allRows)\n",
    "    return []\n",
    "\n",
    "def get_coordinates_from_soup(soup):\n",
    "    # Extract decimal coordinates from the span with class \"geo-dec\"\n",
    "    decimal_coords = soup.find('span', class_='geo-dec')\n",
    "    #display(decimal_coords)\n",
    "    if decimal_coords:\n",
    "        lat, lon = decimal_coords.text.split()\n",
    "        return lat[:-2]  # Remove the last two characters '°N'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "countriesList = df['Country Name'].copy()\n",
    "countriesList = countriesList.replace(\"Georgia\", \"Georgia_(country)\")\n",
    "countriesList = countriesList.replace(\"Micronesia\", \"Federated_States_of_Micronesia\")\n",
    "countriesList = countriesList.unique()\n",
    "\n",
    "religions_dict = {}\n",
    "distanceFromEquator = {}\n",
    "\n",
    "for country in countriesList:\n",
    "    try:\n",
    "        #display(f'Processing {country}')\n",
    "        # get the first table\n",
    "        soup = getSoupFromWiki(country)\n",
    "        table = soup.find('table', class_='infobox ib-country vcard') or soup.find('table', class_='infobox ib-pol-div vcard')\n",
    "\n",
    "        # get the top strip\n",
    "        coords = get_coordinates_from_soup(soup)\n",
    "        if coords:\n",
    "            distanceFromEquator[country] = coords\n",
    "        else:\n",
    "            display(f\"Coordinates for {country} not found!\")\n",
    "        # #display(soup)\n",
    "        # display(table)\n",
    "\n",
    "        if table:\n",
    "        # ------------------------- RELIGION SCRAPING ---------------------------\n",
    "            religions = get_religion_from_table(table)\n",
    "            if religions:\n",
    "                religions_dict[country] = religions\n",
    "            else:\n",
    "                pass\n",
    "                #display(f'No religion found for {country}')\n",
    "        else:\n",
    "            pass\n",
    "            #display(f'No infobox found for {country}')\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #display(f'Some error occurred. Skipping {country}.<br>Error:{e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for debugging the dictionaries\n",
    "def flatten(lst):\n",
    "    '''\n",
    "        Recursively flatten an iterable.\n",
    "    '''\n",
    "    for item in lst:\n",
    "        if isinstance(item, (list, tuple, set, dict)):\n",
    "            yield from flatten(item)\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def getUniqueValuesWithFrequency(d):\n",
    "    '''\n",
    "        Get unique values and their frequencies from a dictionary.\n",
    "    '''\n",
    "    flattened_values = list(flatten(d.values()))\n",
    "    unique_values = set(flattened_values)\n",
    "    frequency = {value: flattened_values.count(value) for value in unique_values}\n",
    "    return frequency\n",
    "\n",
    "\n",
    "def findMatchingKeys_fromDict(d, target_string):\n",
    "    '''\n",
    "        Return a list of Keys that contains the target string\n",
    "    '''\n",
    "    matching_keys = []  # List to store keys that contain the target string\n",
    "    for key, value_list in d.items():\n",
    "        if target_string in value_list:\n",
    "            matching_keys.append(key)\n",
    "    return matching_keys\n",
    "\n",
    "def getEmptyKeys(d):\n",
    "    '''\n",
    "        Return a list of keys with no values or empty lists as values.\n",
    "    '''\n",
    "    return [key for key, value in d.items() if not value or value == []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------- Manual Cleaning of RELIGIONS\n",
    "# Replacing religion on specific countries (manual wiki extraction)\n",
    "religions_dict['Azerbaijan'] = ['islam', 'christianity']\n",
    "religions_dict['Canada'] = ['christianity', 'no religion', 'islam']\n",
    "religions_dict[\"Cote d'Ivoire\"] = ['islam', 'christianity', 'no religion']\n",
    "religions_dict['Egypt'] = ['islam', 'christianity']\n",
    "religions_dict['Eritrea'] = ['christianity', 'islam']\n",
    "religions_dict['Germany'] = ['christianity', 'no religion']\n",
    "religions_dict['Hong Kong'] = ['no religion', 'buddhism', 'christianity']\n",
    "religions_dict['Ireland'] = ['christianity', 'no religion']\n",
    "religions_dict['Israel'] = ['judaism', 'islam']\n",
    "religions_dict['Japan'] = ['no religion', 'buddhism']\n",
    "religions_dict['Macau'] = ['folk', 'buddhism', 'no religion']\n",
    "religions_dict['Nigeria'] = ['christianity', 'islam']\n",
    "religions_dict['Turkey'] = ['islam', 'no religion']\n",
    "religions_dict['United Kingdom'] = ['no religion', 'christianity', 'islam']\n",
    "\n",
    "# Renaming and deleting some religions\n",
    "\n",
    "# # I'll merge some quite different religions on purpose.\n",
    "# # The goal is to have as few significant features as possible\n",
    "# # Given that we'll do one-hot-encoding on them afterwards\n",
    "\n",
    "wordsToReplace = {\n",
    "'animism': 'folk', 'atheist': 'no religion', 'catholicism': 'christianity', 'chondoism': 'folk',\n",
    "'folk religions': 'folk', 'hanafi sunni': 'islam', 'methodism': 'christianity', 'no religion folk': 'no religion',\n",
    "'practicing catholic': 'christianity', 'protestant': 'christianity', 'protestantism': 'christianity',\n",
    "'roman catholic': 'christianity', 'shi a': 'islam', 'shia': 'islam', 'sunni': 'islam', 'sunni islam': 'islam',\n",
    "'tai folk religion': 'folk', 'traditional faiths': 'folk', 'unaffiliated': 'no religion', 'shamanism': 'folk', 'judaism': 'christianity'\n",
    "}\n",
    "\n",
    "wordsToRemove = ['no data', 'no response', 'official']\n",
    "\n",
    "# Replacing religions and removing unwanted words\n",
    "for key, value_list in religions_dict.items():\n",
    "    new_list = []\n",
    "    for word in value_list:\n",
    "        if word not in wordsToRemove:\n",
    "            new_list.append(wordsToReplace.get(word, word))\n",
    "    religions_dict[key] = new_list\n",
    "\n",
    "display(len(religions_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distanceFromEquator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country Area and Exclusive Economic Zone data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "soup = getSoupFromWiki('List_of_countries_and_dependencies_by_area').find('table', class_ = 'wikitable sortable')\n",
    "\n",
    "rows = soup.findAll('tr')\n",
    "\n",
    "dict_countryAreaSize = dict()\n",
    "\n",
    "for row in rows[1:]:\n",
    "    tds = row.findAll('td')\n",
    "    #display(f'processing {row}')\n",
    "\n",
    "    # Storing value | tds[1] is country, tds[2] is total area\n",
    "    if tds and len(tds) > 2:\n",
    "        value = tds[2].text\n",
    "        value_split = value.split('(')[0].strip().replace(',', '')\n",
    "        dict_countryAreaSize[fuzzySearchName(tds[1].text, score_threshold=90)] = value_split\n",
    "\n",
    "# Manually adding some countries\n",
    "dict_countryAreaSize[\"Cote d'Ivoire\"] = 322463\n",
    "dict_countryAreaSize[\"Hong Kong\"] = 2755\n",
    "dict_countryAreaSize[\"Kyrgyz Republic\"] = 199951\n",
    "dict_countryAreaSize[\"Macau\"] = 31\n",
    "dict_countryAreaSize[\"Republic of Congo\"] = 342000\n",
    "dict_countryAreaSize[\"Democratic Republic of Congo\"] = 342000\n",
    "dict_countryAreaSize[\"Slovak Republic\"] = 49037\n",
    "dict_countryAreaSize[\"Brunei Darussalam\"] = 5765\n",
    "dict_countryAreaSize[\"The Bahamas\"] = 13943\n",
    "dict_countryAreaSize[\"The Gambia\"] = 11295\n",
    "dict_countryAreaSize[\"Timor-Leste\"] = 14919\n",
    "dict_countryAreaSize[\"United States\"] = 9833517\n",
    "\n",
    "dict_countryAreaSize.pop(\"PLEASE FILL MANUALLY\", None)\n",
    "\n",
    "display(len(dict_countryAreaSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tables = getSoupFromWiki('Exclusive_economic_zone').findAll('table')[11]\n",
    "rows = tables.findAll('tr')\n",
    "\n",
    "dict_countryEEZ_area = dict()\n",
    "\n",
    "for row in rows[1:]:\n",
    "    tds = row.findAll('td')\n",
    "\n",
    "    value = tds[2].text.strip().split('[')[0].replace(',', '')\n",
    "    if not value:\n",
    "        value = 0\n",
    "\n",
    "    # populating countries and values\n",
    "    dict_countryEEZ_area[fuzzySearchName(tds[1].text.split('[')[0].strip(), score_threshold=90)] = value\n",
    "\n",
    "# Manually adding some countries\n",
    "dict_countryEEZ_area[\"Brunei Darussalam\"] = 10090\n",
    "dict_countryEEZ_area[\"Hong Kong\"] = 20\n",
    "dict_countryEEZ_area[\"Kyrgyz Republic\"] = 0\n",
    "dict_countryEEZ_area[\"Macau\"] = 20\n",
    "dict_countryEEZ_area[\"Micronesia\"] = 2996419\n",
    "dict_countryEEZ_area[\"Slovak Republic\"] = 0\n",
    "dict_countryEEZ_area[\"The Bahamas\"] = 654715\n",
    "dict_countryEEZ_area[\"The Gambia\"] = 23112\n",
    "dict_countryEEZ_area[\"Timor-Leste\"] = 70326\n",
    "\n",
    "dict_countryEEZ_area.pop(\"PLEASE FILL MANUALLY\", None)\n",
    "\n",
    "display(len(dict_countryEEZ_area))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrastructure data collection\n",
    "Now let's collect some basic infrastructure stats. We'll use the latest available numbers from wiki.\n",
    "- Railroads\n",
    "    - RailLength/Country Area (already calculated by wiki)\n",
    "    - % of the total electrified\n",
    "- Ports\n",
    "    - Number of Ports\n",
    "    - Container port traffic per country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndexError('list index out of range')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RAILS\n",
    "table = getSoupFromWiki('List_of_countries_by_rail_transport_network_size').find('table')\n",
    "rows = table.findAll('tr')\n",
    "\n",
    "dict_countryRails = dict()\n",
    "\n",
    "for row in rows[1:]:\n",
    "    tds = row.findAll('td')    \n",
    "\n",
    "    try:\n",
    "        lengthPerArea = float(tds[4].text.strip().replace(',', ''))\n",
    "    \n",
    "        totalElectrified = float(tds[3].text.strip().split('%')[0].replace(',', '')) / 100\n",
    "\n",
    "        #populating countries and values\n",
    "        dict_countryRails[fuzzySearchName(tds[0].text.strip(), score_threshold=90)] = [lengthPerArea, round(totalElectrified, 2)]\n",
    "    except Exception as e:\n",
    "        display(e)\n",
    "        continue\n",
    "\n",
    "dict_countryRails.pop(\"PLEASE FILL MANUALLY\", None)\n",
    "\n",
    "set_A = set(dict_countryRails.keys())\n",
    "set_B = set(refNames['Standard Names'])\n",
    "missingCountries = set_B - set_A\n",
    "\n",
    "for country in missingCountries:\n",
    "    dict_countryRails[country] = [0, 0]\n",
    "\n",
    "display(len(dict_countryRails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ports\n",
    "\n",
    "links = [\n",
    "    'List_of_ports_and_harbours_of_the_Atlantic_Ocean',\n",
    "    'List_of_ports_and_harbours_of_the_Indian_Ocean',\n",
    "    'List_of_ports_and_harbors_of_the_Pacific_Ocean',\n",
    "    'Ports_of_the_Baltic_Sea'\n",
    "]\n",
    "\n",
    "dict_countryPorts = dict()\n",
    "\n",
    "for link in links:\n",
    "    table = getSoupFromWiki(link).findAll('table', class_ = 'wikitable sortable')[0]\n",
    "    rows = table.findAll('tr')\n",
    "\n",
    "    colLocation = 1 if any(word in link for word in ['Indian', 'Baltic']) else 2\n",
    "    #display(link, colLocation)\n",
    "    for row in rows[1:]:\n",
    "        tds = row.findAll('td')\n",
    "        try:\n",
    "            countryName = fuzzySearchName(tds[colLocation].text.split(',')[0].strip())\n",
    "\n",
    "            dict_countryPorts[countryName] = dict_countryPorts.get(countryName, 0) + 1\n",
    "        except Exception as e:\n",
    "            display(e)\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_countryPorts.pop(\"PLEASE FILL MANUALLY\", None)\n",
    "\n",
    "# Manually adding Arctic Ocean ports and Northern Sea Ports\n",
    "dict_countryPorts['Canada'] = dict_countryPorts.get('Canada', 0) + 2\n",
    "dict_countryPorts['United States'] = dict_countryPorts.get('United States', 0) + 1\n",
    "dict_countryPorts['Iceland'] = dict_countryPorts.get('Iceland', 0) + 1\n",
    "dict_countryPorts['Russia'] = dict_countryPorts.get('Russia', 0) + 13\n",
    "dict_countryPorts['Norway'] = dict_countryPorts.get('Norway', 0) + 5\n",
    "dict_countryPorts['Belgium'] = dict_countryPorts.get('Belgium', 0) + 6\n",
    "dict_countryPorts['Denmark'] = dict_countryPorts.get('Denmark', 0) + 13\n",
    "dict_countryPorts['France'] = dict_countryPorts.get('France', 0) + 1\n",
    "dict_countryPorts['Germany'] = dict_countryPorts.get('Germany', 0) + 11\n",
    "dict_countryPorts['Netherlands'] = dict_countryPorts.get('Netherlands', 0) + 14\n",
    "dict_countryPorts['Norway'] = dict_countryPorts.get('Norway', 0) + 13\n",
    "dict_countryPorts['United Kingdom'] = dict_countryPorts.get('United Kingdom', 0) + 22\n",
    "\n",
    "# Manually adding port number (basic google research) // or from https://www.searates.com/maritime/\n",
    "dict_countryPorts['Comoros'] = 3\n",
    "dict_countryPorts['Dominica'] = 2\n",
    "dict_countryPorts['Fiji'] = 5\n",
    "dict_countryPorts['Hong Kong'] = 1\n",
    "dict_countryPorts['Jordan'] = 1\n",
    "dict_countryPorts['Kiribati'] = 5\n",
    "dict_countryPorts['Macau'] = 1\n",
    "dict_countryPorts['Micronesia'] = 1\n",
    "dict_countryPorts['Republic of Congo'] = 4\n",
    "dict_countryPorts['Samoa'] = 1\n",
    "dict_countryPorts['Sao Tome and Principe'] = 2\n",
    "dict_countryPorts['Timor-Leste'] = 11\n",
    "dict_countryPorts['Tonga'] = 2\n",
    "dict_countryPorts['Vanuatu'] = 2\n",
    "\n",
    "# Country without a port, but has access to the sea\n",
    "dict_countryPorts['Bosnia and Herzegovina'] = 0\n",
    "\n",
    "\n",
    "\n",
    "landLocked = list(df_landLockedCountries[df_landLockedCountries['isLandLocked'] == True]['Country Name'])\n",
    "\n",
    "# Filling landlocked countries with \n",
    "for country in landLocked:\n",
    "    dict_countryPorts[country] = 0\n",
    "\n",
    "display(len(dict_countryPorts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining all the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = pd.merge(df_CountryNeigh, df_landLockedCountries, on='Country Name', how='outer')\n",
    "\n",
    "df_rails = pd.DataFrame(list(dict_countryRails.items()), columns=['Country Name', 'Values'])\n",
    "df_rails[['Rail Density', 'Pctg of Rail Electrified']] = pd.DataFrame(df_rails.Values.tolist(), index=df_rails.index)\n",
    "\n",
    "df_master = pd.merge(df_master, df_rails, on='Country Name', how='left')\n",
    "\n",
    "df_master['Warred Against'] = df_master['Country Name'].map(opponentsByCountry)\n",
    "df_master['Area Size (km2)'] = df_master['Country Name'].map(dict_countryAreaSize)\n",
    "df_master['Expanded EconZone Area'] = df_master['Country Name'].map(dict_countryEEZ_area)\n",
    "df_master['Amount of Ports'] = df_master['Country Name'].map(dict_countryPorts)\n",
    "df_master['Distance from Equator'] = df_master['Country Name'].map(distanceFromEquator)\n",
    "df_master['Majoritary Religions'] = df_master['Country Name'].map(religions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master.sort_values(by=\"Country Name\", inplace=True)\n",
    "\n",
    "# Ensure 'Country Name' is the first column\n",
    "cols = ['Country Name'] + [col for col in df_master.columns if col != 'Country Name']\n",
    "df_master = df_master[cols]\n",
    "\n",
    "df_master.to_csv(f'..\\Data_Sets\\processed\\\\addData_fromWiki.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EconStats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
